{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import urllib.request\n",
    "import os\n",
    "import urllib.parse\n",
    "import shutil\n",
    "from io import BytesIO\n",
    "\n",
    "#import cv2\n",
    "from PIL import Image\n",
    "import time\n",
    "import tensorflow.keras\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeBlogKeywordList = [\"스토리앤\",\"seoulouba\",\"mateb.kr\",\"revu\",\"weble\",\"ohmyblog\",\"mrblog\",\"tble\",\n",
    "    \"dinnerqueen\",\"%EA%B3%B5%EC%A0%95%EA%B1%B0%EB%9E%98%EC%9C%84%EC%9B%90%ED%9A%8C-%EB%AC%B8%EA%B5%AC\",\n",
    "    \"%EB%B8%94%EB%A1%9C%EA%B7%B8%EC%9B%90%EC%A0%95%EB%8C%80\",\n",
    "    \"banner_\",\"%EC%9D%B4%EC%8A%88%EB%B8%94%EB%A1%9C%EA%B7%B8\",\n",
    "    \"%EB%B0%B0%EB%84%88\",\"http://echoblog.net/images/sponsor-banner.png\",\n",
    "                      \"sponsor\",\"banner\",\"echoblog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#define const value\n",
    "batch_size = 3000\n",
    "epochs = 15\n",
    "IMG_HEIGHT = 260\n",
    "IMG_WIDTH = 460\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(32, 3, padding='same', activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_diff_width_0624_v1(batch-3000)/cp-0015.ckpt\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "checkpoint_path = \"model_diff_width_0624_v1(batch-3000)/cp-0015.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "print(latest)\n",
    "\n",
    "# create model\n",
    "model = create_model()\n",
    "model.load_weights(latest)\n",
    "\n",
    "data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)\n",
    "teachable_model = tensorflow.keras.models.load_model('/Users/esens/Downloads/converted_keras/keras_model.h5')\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictionWithoutDownload(uri):\n",
    "    start_time = time.time();\n",
    "    filename = uri.split('/')[-1].split('?')[0];\n",
    "    img_url2 = urllib.parse.quote_plus(str(filename))\n",
    "    img_url_final = uri.replace(uri.split('/')[-1].split('?')[0],img_url2)\n",
    "    \n",
    "#     response = requests.get(uri)\n",
    "    response = urllib.request.urlopen(img_url_final).read()\n",
    "#     print(\"---[1] {}s seconds---\".format(time.time()-start_time,\".2f\"))\n",
    "    start_time = time.time();\n",
    "    #image = tf.keras.preprocessing.image.load_img(image_path,target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "#     img = Image.open(BytesIO(response.content))\n",
    "    img = Image.open(BytesIO(response))\n",
    "#     print(\"---[2] {}s seconds---\".format(time.time()-start_time,\".2f\"))\n",
    "    start_time = time.time();\n",
    "    #width,height\n",
    "    resize_img = img.resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "#     print(\"---[3] {}s seconds---\".format(time.time()-start_time,\".2f\"))\n",
    "    start_time = time.time();\n",
    "\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(resize_img)\n",
    "    input_arr = np.array([input_arr])  # Convert single image to a batch.\n",
    "#     print(\"---[4] {}s seconds---\".format(time.time()-start_time,\".2f\"))\n",
    "    start_time = time.time();\n",
    "    \n",
    "    predictions_class = model.predict_classes(input_arr)\n",
    "#     print(\"---[5] {}s seconds---\".format((time.time()-start_time),\".2f\"))\n",
    "\n",
    "#     predictions = model.predict(input_arr)\n",
    "    print(\"========pridiction[Without Download]========\");\n",
    "#     print(test_data_gen.class_indices)\n",
    "    print(\"predictions class : \" , predictions_class);\n",
    "#     print(\"predictions : \" , predictions)\n",
    "    return predictions_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================================\n",
    "#이미지 분류 [teachable machine model]\n",
    "def teachableMachinePrediction(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        size = (224, 224)\n",
    "        image = ImageOps.fit(image, size, Image.ANTIALIAS)\n",
    "        resize = Image.open(image_path);\n",
    "        print(\"image size : \",resize.size);\n",
    "        \n",
    "        image_array = np.asarray(image)\n",
    "        normalized_image_array = (image_array.astype(np.float32) / 127.0) - 1\n",
    "        newdata = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)    \n",
    "        \n",
    "#         data = np.array([np.array(cv2.imread(image_path[i])) for i in range(len(image_path))])\n",
    "#         features = data.flatten().reshape(1, 224,224,3)\n",
    "#         print(\"features : \" , features.shape)\n",
    "\n",
    "        if(normalized_image_array.shape == (224,224,4)):\n",
    "            print('224,224,4 : ' , image_path)\n",
    "            print(normalized_image_array.shape)\n",
    "            normalized_image_array_reshape = normalized_image_array.reshape(224,224,3)\n",
    "            print(normalized_image_array_reshape.shape)\n",
    "\n",
    "        elif (normalized_image_array.shape == (224, 224)):\n",
    "            print('224,224 : ', image_path)\n",
    "            normalized_image_array_reshape = normalized_image_array.reshape(-1,224,224,3)\n",
    "            print(normalized_image_array_reshape.shape)\n",
    "\n",
    "        else:\n",
    "            normalized_image_array_reshape = normalized_image_array.reshape(1,224,224,3)\n",
    "\n",
    "        # Load the image into the array\n",
    "        newdata[0] = normalized_image_array_reshape\n",
    "        #todo image shape 다른경우 처리해줘야함(png일 때 로 추측됨..)\n",
    "        print('newdata.shape : ' , newdata.shape)\n",
    "  \n",
    "        #predictions class \n",
    "        # {0 : advertise , 1: others}\n",
    "        \n",
    "        prediction = teachable_model.predict(newdata)\n",
    "        prediction_class = teachable_model.predict_classes(newdata)\n",
    "        #print(\"========pridiction[teachable]========\")\n",
    "        #print(prediction_class)\n",
    "    except Exception as ex:\n",
    "        print(\"predict error[teachable machine] : \" , ex)\n",
    "        \n",
    "    return prediction_class\n",
    "\n",
    "    \n",
    "# =====================================================================\n",
    "#이미지 분류 [my model]\n",
    "\n",
    "def imageClassification(image_path):\n",
    "    image = tf.keras.preprocessing.image.load_img(image_path,target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "#    print(\"input_arr type : \" ,type(input_arr))\n",
    "    input_arr = np.array([input_arr])  # Convert single image to a batch.\n",
    "\n",
    "    predictions_class = model.predict_classes(input_arr)\n",
    "    predictions = model.predict(input_arr)\n",
    "\n",
    "    print(\"========pridiction========\");\n",
    "#     print(test_data_gen.class_indices)\n",
    "    print(\"predictions class : \" , predictions_class);\n",
    "    print(\"predictions : \" , predictions);\n",
    "#    print(\"predictions argmax : \",np.argmax(predictions[0])); # same way\n",
    "    return predictions_class;\n",
    "\n",
    "# =====================================================================\n",
    "#이미지 저장\n",
    "def saveImage(div_option,index,img_url,nickName,postNumber):\n",
    "    #블로그 유저 별로 폴더를 나눌건지에 대한 옵션\n",
    "    #true -> path : image/{nickName}/{postNumber}\n",
    "    #false -> path : image/myimg/\n",
    "    if(div_option):\n",
    "        if nickName is \"\":\n",
    "            nickName = \"unknown\"\n",
    "        if postNumber is \"\":\n",
    "            postNumber = \"1\"        \n",
    "        imgPath = f'image/{nickName}/{postNumber}/{index}.jpg'\n",
    "        dirPath = f'image/{nickName}/{postNumber}'\n",
    "    #폴더별로 분기하지 않을떄 path : image/myimg/\n",
    "    else:\n",
    "        list = os.listdir(\"image/temp\")\n",
    "        last_index = len(list)\n",
    "#         print(\"last_index : \",last_index)\n",
    "        dirPath = 'image/temp'\n",
    "        imgPath = f'image/temp/{last_index}.jpg'\n",
    "    \n",
    "    if not os.path.isdir(dirPath):\n",
    "        #폴더 생성\n",
    "        try:\n",
    "            os.makedirs(dirPath)\n",
    "        except Exception as ex:\n",
    "            print(\"error catch : \" , ex)\n",
    "                \n",
    "    #파일 저장\n",
    "    #temp에 저장 -> classification -> 분류 저장\n",
    "    \n",
    "    try:\n",
    "        filename = img_url.split('/')[-1].split('?')[0];\n",
    "        img_url2 = urllib.parse.quote_plus(str(filename))\n",
    "        img_url_final = img_url.replace(img_url.split('/')[-1].split('?')[0],img_url2)\n",
    "#         print(\"img_url_final : \" ,img_url_final);\n",
    "        for item in fakeBlogKeywordList:\n",
    "            if(img_url_final.find(item) > 0):\n",
    "                print('this is fake img')\n",
    "\n",
    "                imgPath = f\"image/temp/fake_{last_index}.jpg\"\n",
    "                break;\n",
    "        \n",
    "        urllib.request.urlretrieve(img_url_final, imgPath)\n",
    "        #분류\n",
    "        #{'advertise': 0, 'image': 1}\n",
    "        my_prediction  = imageClassification(imgPath);\n",
    "        #my_prediction  = teachableMachinePrediction(imgPath);\n",
    "#         print(\"prediction : \", my_prediction[0]);\n",
    "        \n",
    "        if (my_prediction[0] == 0):\n",
    "            list = os.listdir(\"ad_img\")\n",
    "            last_index = len(list)\n",
    "            #os.rename(imgPath, f'ad_img/{last_index}.jpg')\n",
    "            #shutil.move(imgPath, f'ad_img/{last_index}.jpg')\n",
    "            os.replace(imgPath, f'ad_img/{last_index}.jpg')\n",
    "            return False\n",
    "        return True\n",
    "    except Exception as ex:\n",
    "        print(\"img save error : \" , ex)\n",
    "        return False\n",
    "    \n",
    "    \n",
    "# =====================================================================\n",
    "# image gethering\n",
    "def findBlogImgGethering(searchQuery, searchOption, page):\n",
    "    try:\n",
    "        start_time = time.time();\n",
    "\n",
    "        url = f'https://search.naver.com/search.naver?query={searchQuery}&sm=tab_pge&srchby=all&st={searchOption}&where=post&start={page}'\n",
    "        html = requests.get(url)\n",
    "        # 1차, blog URL 찾기\n",
    "        soup = BeautifulSoup(html.text, 'html.parser')\n",
    "        index = 0;\n",
    "\n",
    "        url_list = [];\n",
    "        for li_item in soup.find_all('li',{'class' : 'sh_blog_top'}):\n",
    "            child_item = li_item.find('a',{'class' : 'sh_blog_title'})\n",
    "            title = child_item.attrs['title']\n",
    "            href = child_item.attrs['href']\n",
    "\n",
    "            #url list 저장\n",
    "            url_list.append(href)\n",
    "\n",
    "            print(f'title : {title} , href : {href}')\n",
    "            print('----------------------------------')\n",
    "\n",
    "        # url 파싱 및 예외처리\n",
    "        for url_item in url_list:\n",
    "            nickName = \"\";\n",
    "            postNumber = \"\";\n",
    "\n",
    "            if(url_item.find(\"blog.me\") > 0):\n",
    "                parsing = url_item;\n",
    "                parsing = parsing.replace(\"https://\",\"\")\n",
    "                nickName = parsing.split('.')[0]\n",
    "                postNumber = parsing.split('/')[1]\n",
    "                blogUrl = \"https://m.blog.naver.com/\" + nickName + \"?Redirect=Log&logNo=\" + postNumber\n",
    "\n",
    "            else:\n",
    "                blogUrl = url_item.replace(\"https://\", \"https://m.\");\n",
    "\n",
    "            nickName = blogUrl.split('/')[-1].split('?')[0]\n",
    "            postNumber = blogUrl.split('/')[-1].split('?')[1].split('=')[-1]\n",
    "\n",
    "            if nickName is \"\":\n",
    "                nickName = \"unknown\"\n",
    "            if postNumber is \"\":\n",
    "                postNumber = \"1\"\n",
    "\n",
    "            print(\"------ digging more -------\")\n",
    "            print(\"nickName : \" , blogUrl.split('/')[-1].split('?')[0])\n",
    "            print(\"postNumber : \" , blogUrl.split('/')[-1].split('?')[1].split('=')[-1])\n",
    "            print(\"blogUrl : \" , blogUrl)        \n",
    "            start_time = time.time();\n",
    "            #전체 목록 순회\n",
    "            blog_html = requests.get(blogUrl)\n",
    "            blog_soup = BeautifulSoup(blog_html.text, 'html.parser')\n",
    "            blog_image_class = blog_soup.find_all('div',{'class' : 'se-image'});\n",
    "\n",
    "            for div_obj in blog_image_class:\n",
    "                for idx, img_item in enumerate(div_obj.find_all('img')):\n",
    "                    img_url = img_item.attrs['src'];\n",
    "\n",
    "                    if(img_url.find(\"w80_blur\") > 0):\n",
    "                        img_url = img_url.replace(\"w80_blur\" , \"w800\")\n",
    "                        #np array after save image(for crawling data)\n",
    "                    #isSaveSuccess = saveImage(False,index,img_url,nickName,postNumber)\n",
    "                    #np arr from img_url for validation\n",
    "                    PredictionWithoutDownload(img_url)\n",
    "    #                 if(isSaveSuccess):  \n",
    "    #                     index = index+1\n",
    "    #         print(\"---{}s seconds---\".format(time.time()-start_time),\".2f\")\n",
    "    except Exception as ex:\n",
    "        print(\"error catch : \" , ex)\n",
    "        print(\"url : \" ,img_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title : 기가 막혔던 종각 맛집 , href : https://forurwinter.blog.me/221988900945\n",
      "----------------------------------\n",
      "title : 푸짐했던 종각 맛집 , href : https://blog.naver.com/celinicious?Redirect=Log&logNo=222006262433\n",
      "----------------------------------\n",
      "title : 여기가 핫플! 종각 맛집 , href : https://blog.naver.com/limmmm22?Redirect=Log&logNo=222012963389\n",
      "----------------------------------\n",
      "title : 가성비 쩔었던 종각 맛집, 교대이층집 , href : https://blog.naver.com/el512?Redirect=Log&logNo=222008681533\n",
      "----------------------------------\n",
      "title : 0324 종각 고기집/콜키지프리 맛집 고메식당 , href : https://blog.naver.com/p_radn?Redirect=Log&logNo=221878088433\n",
      "----------------------------------\n",
      "title : 믿고갔던 종각 맛집 , href : https://blog.naver.com/porky0122?Redirect=Log&logNo=221991050525\n",
      "----------------------------------\n",
      "title : 종각 맛집 매콤한 파스타 , href : https://blog.naver.com/kangmj1992?Redirect=Log&logNo=222011821916\n",
      "----------------------------------\n",
      "title : 최초였던 종각 맛집 , href : https://blog.naver.com/minjae0413?Redirect=Log&logNo=222018153429\n",
      "----------------------------------\n",
      "title : 끝장난 종각 맛집 , href : https://ssomerry.blog.me/221983614903\n",
      "----------------------------------\n",
      "title : 까리했던 종각 맛집 , href : https://blog.naver.com/ju_love1202?Redirect=Log&logNo=221952793333\n",
      "----------------------------------\n",
      "------ digging more -------\n",
      "nickName :  forurwinter\n",
      "postNumber :  221988900945\n",
      "blogUrl :  https://m.blog.naver.com/forurwinter?Redirect=Log&logNo=221988900945\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "------ digging more -------\n",
      "nickName :  celinicious\n",
      "postNumber :  222006262433\n",
      "blogUrl :  https://m.blog.naver.com/celinicious?Redirect=Log&logNo=222006262433\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[0]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "------ digging more -------\n",
      "nickName :  limmmm22\n",
      "postNumber :  222012963389\n",
      "blogUrl :  https://m.blog.naver.com/limmmm22?Redirect=Log&logNo=222012963389\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "------ digging more -------\n",
      "nickName :  el512\n",
      "postNumber :  222008681533\n",
      "blogUrl :  https://m.blog.naver.com/el512?Redirect=Log&logNo=222008681533\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[0]]\n",
      "------ digging more -------\n",
      "nickName :  p_radn\n",
      "postNumber :  221878088433\n",
      "blogUrl :  https://m.blog.naver.com/p_radn?Redirect=Log&logNo=221878088433\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "error catch :  Error when checking input: expected conv2d_input to have shape (260, 460, 3) but got array with shape (260, 460, 1)\n",
      "url :  https://mblogthumb-phinf.pstatic.net/MjAyMDAzMjhfMzQg/MDAxNTg1MzY0NjcyNjY3.3kfk4WR3MhB-phe8wpVGPt4Bwahv_f9rdCeTTj0aPdcg.8Ttg7OOMtwDHRi7pUz8ZSQWcSMPbqlNb7sPmiPYa0aIg.GIF.p_radn/KakaoTalk_20200328_112418316_01.gif?type=w800\n"
     ]
    }
   ],
   "source": [
    "searchQuery = \"종각 맛집\"\n",
    "searchOption = 'sim'  #sim or date  \n",
    "page = 1\n",
    "\n",
    "findBlogImgGethering(searchQuery,searchOption,page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
