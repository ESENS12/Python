{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import urllib.request\n",
    "import os\n",
    "import urllib.parse\n",
    "import shutil\n",
    "from io import BytesIO\n",
    "\n",
    "#import cv2\n",
    "from PIL import Image\n",
    "import time\n",
    "import tensorflow.keras\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeBlogKeywordList = [\"스토리앤\",\"seoulouba\",\"mateb.kr\",\"revu\",\"weble\",\"ohmyblog\",\"mrblog\",\"tble\",\n",
    "    \"dinnerqueen\",\"%EA%B3%B5%EC%A0%95%EA%B1%B0%EB%9E%98%EC%9C%84%EC%9B%90%ED%9A%8C-%EB%AC%B8%EA%B5%AC\",\n",
    "    \"%EB%B8%94%EB%A1%9C%EA%B7%B8%EC%9B%90%EC%A0%95%EB%8C%80\",\n",
    "    \"banner_\",\"%EC%9D%B4%EC%8A%88%EB%B8%94%EB%A1%9C%EA%B7%B8\",\n",
    "    \"%EB%B0%B0%EB%84%88\",\"http://echoblog.net/images/sponsor-banner.png\",\n",
    "                      \"sponsor\",\"banner\",\"echoblog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#define const value\n",
    "batch_size = 3000\n",
    "epochs = 15\n",
    "IMG_HEIGHT = 260\n",
    "IMG_WIDTH = 460\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(32, 3, padding='same', activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_diff_width_0624_v1(batch-3000)/cp-0015.ckpt\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "checkpoint_path = \"model_diff_width_0624_v1(batch-3000)/cp-0015.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "print(latest)\n",
    "\n",
    "# create model\n",
    "model = create_model()\n",
    "model.load_weights(latest)\n",
    "\n",
    "data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)\n",
    "teachable_model = tensorflow.keras.models.load_model('/Users/esens/Downloads/converted_keras/keras_model.h5')\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictionWithoutDownload(uri):\n",
    "    response = requests.get(uri)\n",
    "    \n",
    "    #image = tf.keras.preprocessing.image.load_img(image_path,target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    #width,height\n",
    "    resize_img = img.resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(resize_img)\n",
    "    input_arr = np.array([input_arr])  # Convert single image to a batch.\n",
    "    \n",
    "    predictions_class = model.predict_classes(input_arr)\n",
    "    predictions = model.predict(input_arr)\n",
    "    print(\"========pridiction[Without Download]========\");\n",
    "#     print(test_data_gen.class_indices)\n",
    "    print(\"predictions class : \" , predictions_class);\n",
    "    print(\"predictions : \" , predictions);\n",
    "    return predictions_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================================\n",
    "#이미지 분류 [teachable machine model]\n",
    "def teachableMachinePrediction(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        size = (224, 224)\n",
    "        image = ImageOps.fit(image, size, Image.ANTIALIAS)\n",
    "        resize = Image.open(image_path);\n",
    "        print(\"image size : \",resize.size);\n",
    "        \n",
    "        image_array = np.asarray(image)\n",
    "        normalized_image_array = (image_array.astype(np.float32) / 127.0) - 1\n",
    "        newdata = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)    \n",
    "        \n",
    "#         data = np.array([np.array(cv2.imread(image_path[i])) for i in range(len(image_path))])\n",
    "#         features = data.flatten().reshape(1, 224,224,3)\n",
    "#         print(\"features : \" , features.shape)\n",
    "\n",
    "        if(normalized_image_array.shape == (224,224,4)):\n",
    "            print('224,224,4 : ' , image_path)\n",
    "            print(normalized_image_array.shape)\n",
    "            normalized_image_array_reshape = normalized_image_array.reshape(224,224,3)\n",
    "            print(normalized_image_array_reshape.shape)\n",
    "\n",
    "        elif (normalized_image_array.shape == (224, 224)):\n",
    "            print('224,224 : ', image_path)\n",
    "            normalized_image_array_reshape = normalized_image_array.reshape(-1,224,224,3)\n",
    "            print(normalized_image_array_reshape.shape)\n",
    "\n",
    "        else:\n",
    "            normalized_image_array_reshape = normalized_image_array.reshape(1,224,224,3)\n",
    "\n",
    "        # Load the image into the array\n",
    "        newdata[0] = normalized_image_array_reshape\n",
    "        #todo image shape 다른경우 처리해줘야함(png일 때 로 추측됨..)\n",
    "        print('newdata.shape : ' , newdata.shape)\n",
    "  \n",
    "        #predictions class \n",
    "        # {0 : advertise , 1: others}\n",
    "        \n",
    "        prediction = teachable_model.predict(newdata)\n",
    "        prediction_class = teachable_model.predict_classes(newdata)\n",
    "        #print(\"========pridiction[teachable]========\")\n",
    "        #print(prediction_class)\n",
    "    except Exception as ex:\n",
    "        print(\"predict error[teachable machine] : \" , ex)\n",
    "        \n",
    "    return prediction_class\n",
    "\n",
    "    \n",
    "# =====================================================================\n",
    "#이미지 분류 [my model]\n",
    "\n",
    "def imageClassification(image_path):\n",
    "    image = tf.keras.preprocessing.image.load_img(image_path,target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "#    print(\"input_arr type : \" ,type(input_arr))\n",
    "    input_arr = np.array([input_arr])  # Convert single image to a batch.\n",
    "\n",
    "    predictions_class = model.predict_classes(input_arr)\n",
    "    predictions = model.predict(input_arr)\n",
    "\n",
    "    print(\"========pridiction========\");\n",
    "#     print(test_data_gen.class_indices)\n",
    "    print(\"predictions class : \" , predictions_class);\n",
    "    print(\"predictions : \" , predictions);\n",
    "#    print(\"predictions argmax : \",np.argmax(predictions[0])); # same way\n",
    "    return predictions_class;\n",
    "\n",
    "# =====================================================================\n",
    "#이미지 저장\n",
    "def saveImage(div_option,index,img_url,nickName,postNumber):\n",
    "    #블로그 유저 별로 폴더를 나눌건지에 대한 옵션\n",
    "    #true -> path : image/{nickName}/{postNumber}\n",
    "    #false -> path : image/myimg/\n",
    "    if(div_option):\n",
    "        if nickName is \"\":\n",
    "            nickName = \"unknown\"\n",
    "        if postNumber is \"\":\n",
    "            postNumber = \"1\"        \n",
    "        imgPath = f'image/{nickName}/{postNumber}/{index}.jpg'\n",
    "        dirPath = f'image/{nickName}/{postNumber}'\n",
    "    #폴더별로 분기하지 않을떄 path : image/myimg/\n",
    "    else:\n",
    "        list = os.listdir(\"image/temp\")\n",
    "        last_index = len(list)\n",
    "#         print(\"last_index : \",last_index)\n",
    "        dirPath = 'image/temp'\n",
    "        imgPath = f'image/temp/{last_index}.jpg'\n",
    "    \n",
    "    if not os.path.isdir(dirPath):\n",
    "        #폴더 생성\n",
    "        try:\n",
    "            os.makedirs(dirPath)\n",
    "        except Exception as ex:\n",
    "            print(\"error catch : \" , ex)\n",
    "                \n",
    "    #파일 저장\n",
    "    #temp에 저장 -> classification -> 분류 저장\n",
    "    \n",
    "    try:\n",
    "        filename = img_url.split('/')[-1].split('?')[0];\n",
    "        img_url2 = urllib.parse.quote_plus(str(filename))\n",
    "        img_url_final = img_url.replace(img_url.split('/')[-1].split('?')[0],img_url2)\n",
    "#         print(\"img_url_final : \" ,img_url_final);\n",
    "        for item in fakeBlogKeywordList:\n",
    "            if(img_url_final.find(item) > 0):\n",
    "                print('this is fake img')\n",
    "\n",
    "                imgPath = f\"image/temp/fake_{last_index}.jpg\"\n",
    "                break;\n",
    "        \n",
    "        urllib.request.urlretrieve(img_url_final, imgPath)\n",
    "        #분류\n",
    "        #{'advertise': 0, 'image': 1}\n",
    "        my_prediction  = imageClassification(imgPath);\n",
    "        #my_prediction  = teachableMachinePrediction(imgPath);\n",
    "#         print(\"prediction : \", my_prediction[0]);\n",
    "        \n",
    "        if (my_prediction[0] == 0):\n",
    "            list = os.listdir(\"ad_img\")\n",
    "            last_index = len(list)\n",
    "            #os.rename(imgPath, f'ad_img/{last_index}.jpg')\n",
    "            #shutil.move(imgPath, f'ad_img/{last_index}.jpg')\n",
    "            os.replace(imgPath, f'ad_img/{last_index}.jpg')\n",
    "            return False\n",
    "        return True\n",
    "    except Exception as ex:\n",
    "        print(\"img save error : \" , ex)\n",
    "        return False\n",
    "    \n",
    "    \n",
    "# =====================================================================\n",
    "# image gethering\n",
    "def findBlogImgGethering(searchQuery, searchOption, page):\n",
    "    start_time = time.time();\n",
    "    \n",
    "    url = f'https://search.naver.com/search.naver?query={searchQuery}&sm=tab_pge&srchby=all&st={searchOption}&where=post&start={page}'\n",
    "    html = requests.get(url)\n",
    "    # 1차, blog URL 찾기\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    index = 0;\n",
    "\n",
    "    url_list = [];\n",
    "    for li_item in soup.find_all('li',{'class' : 'sh_blog_top'}):\n",
    "        child_item = li_item.find('a',{'class' : 'sh_blog_title'})\n",
    "        title = child_item.attrs['title']\n",
    "        href = child_item.attrs['href']\n",
    "\n",
    "        #url list 저장\n",
    "        url_list.append(href)\n",
    "\n",
    "        print(f'title : {title} , href : {href}')\n",
    "        print('----------------------------------')\n",
    "        \n",
    "    # url 파싱 및 예외처리\n",
    "    for url_item in url_list:\n",
    "        nickName = \"\";\n",
    "        postNumber = \"\";\n",
    "\n",
    "        if(url_item.find(\"blog.me\") > 0):\n",
    "            parsing = url_item;\n",
    "            parsing = parsing.replace(\"https://\",\"\")\n",
    "            nickName = parsing.split('.')[0]\n",
    "            postNumber = parsing.split('/')[1]\n",
    "            blogUrl = \"https://m.blog.naver.com/\" + nickName + \"?Redirect=Log&logNo=\" + postNumber\n",
    "\n",
    "        else:\n",
    "            blogUrl = url_item.replace(\"https://\", \"https://m.\");\n",
    "\n",
    "        nickName = blogUrl.split('/')[-1].split('?')[0]\n",
    "        postNumber = blogUrl.split('/')[-1].split('?')[1].split('=')[-1]\n",
    "\n",
    "        if nickName is \"\":\n",
    "            nickName = \"unknown\"\n",
    "        if postNumber is \"\":\n",
    "            postNumber = \"1\"\n",
    "            \n",
    "        print(\"------ digging more -------\")\n",
    "        print(\"nickName : \" , blogUrl.split('/')[-1].split('?')[0])\n",
    "        print(\"postNumber : \" , blogUrl.split('/')[-1].split('?')[1].split('=')[-1])\n",
    "        print(\"blogUrl : \" , blogUrl)        \n",
    "\n",
    "        #전체 목록 순회\n",
    "        blog_html = requests.get(blogUrl)\n",
    "        blog_soup = BeautifulSoup(blog_html.text, 'html.parser')\n",
    "        blog_image_class = blog_soup.find_all('div',{'class' : 'se-image'});\n",
    "\n",
    "        for div_obj in blog_image_class:\n",
    "            for idx, img_item in enumerate(div_obj.find_all('img')):\n",
    "                img_url = img_item.attrs['src'];\n",
    "\n",
    "                if(img_url.find(\"w80_blur\") > 0):\n",
    "                    img_url = img_url.replace(\"w80_blur\" , \"w800\")\n",
    "                    #np array after save image(for crawling data)\n",
    "                #isSaveSuccess = saveImage(False,index,img_url,nickName,postNumber)\n",
    "                #np arr from img_url for validation\n",
    "                PredictionWithoutDownload(img_url)\n",
    "#                 if(isSaveSuccess):  \n",
    "#                     index = index+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title : 기가 막혔던 종각 맛집 , href : https://forurwinter.blog.me/221988900945\n",
      "----------------------------------\n",
      "title : 푸짐했던 종각 맛집 , href : https://blog.naver.com/celinicious?Redirect=Log&logNo=222006262433\n",
      "----------------------------------\n",
      "title : 여기가 핫플! 종각 맛집 , href : https://blog.naver.com/limmmm22?Redirect=Log&logNo=222012963389\n",
      "----------------------------------\n",
      "title : 종각 맛집 매콤한 파스타 , href : https://blog.naver.com/kangmj1992?Redirect=Log&logNo=222011821916\n",
      "----------------------------------\n",
      "title : 가성비 쩔었던 종각 맛집, 교대이층집 , href : https://blog.naver.com/el512?Redirect=Log&logNo=222008681533\n",
      "----------------------------------\n",
      "title : 0324 종각 고기집/콜키지프리 맛집 고메식당 , href : https://blog.naver.com/p_radn?Redirect=Log&logNo=221878088433\n",
      "----------------------------------\n",
      "title : 믿고갔던 종각 맛집 , href : https://blog.naver.com/porky0122?Redirect=Log&logNo=221991050525\n",
      "----------------------------------\n",
      "title : 끝장난 종각 맛집 , href : https://ssomerry.blog.me/221983614903\n",
      "----------------------------------\n",
      "title : 흐뭇했던 종각 맛집 , href : https://blog.naver.com/hoon1033?Redirect=Log&logNo=221995568472\n",
      "----------------------------------\n",
      "title : 까리했던 종각 맛집 , href : https://blog.naver.com/ju_love1202?Redirect=Log&logNo=221952793333\n",
      "----------------------------------\n",
      "------ digging more -------\n",
      "nickName :  forurwinter\n",
      "postNumber :  221988900945\n",
      "blogUrl :  https://m.blog.naver.com/forurwinter?Redirect=Log&logNo=221988900945\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2017.6284]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2379.8948]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2804.8228]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1893.276]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2802.584]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2088.3665]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2534.6956]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2313.3667]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1581.9243]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2184.6648]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2700.2522]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[3007.8318]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[3004.198]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2835.8547]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2920.6067]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2776.1172]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2723.4412]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2563.8914]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1960.1575]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2274.7354]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2018.9869]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1548.6798]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2598.733]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[3045.9744]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[3062.785]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2004.133]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2704.17]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2131.3206]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2093.1084]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2406.9065]]\n",
      "------ digging more -------\n",
      "nickName :  celinicious\n",
      "postNumber :  222006262433\n",
      "blogUrl :  https://m.blog.naver.com/celinicious?Redirect=Log&logNo=222006262433\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[0]]\n",
      "predictions :  [[-534.2382]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1354.8107]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1529.0548]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1347.3492]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2089.5]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1787.702]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[506.48254]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[676.73444]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1450.1381]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[641.12946]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1466.0756]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1393.2229]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1366.2745]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1703.3687]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1689.3053]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1637.4115]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2232.2004]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[995.7817]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2426.4385]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1378.3043]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1155.9603]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2403.664]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2454.1377]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1603.8479]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1572.5846]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1604.2106]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1304.7163]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1444.2218]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1600.4727]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1051.3246]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[471.80008]]\n",
      "------ digging more -------\n",
      "nickName :  limmmm22\n",
      "postNumber :  222012963389\n",
      "blogUrl :  https://m.blog.naver.com/limmmm22?Redirect=Log&logNo=222012963389\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[818.1519]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[538.25525]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1753.8689]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[750.2352]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1389.7719]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[230.06778]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[855.91504]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[173.25085]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1837.6031]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1630.7871]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1094.6621]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1036.2961]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1219.8767]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1852.2229]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2135.777]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2457.5615]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1503.2554]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1510.3912]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[953.58887]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1751.086]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2153.2434]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2525.9685]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1298.5007]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1344.971]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[655.96344]]\n",
      "------ digging more -------\n",
      "nickName :  kangmj1992\n",
      "postNumber :  222011821916\n",
      "blogUrl :  https://m.blog.naver.com/kangmj1992?Redirect=Log&logNo=222011821916\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1173.5107]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[927.9847]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1256.422]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[311.96518]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1815.7983]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[0]]\n",
      "predictions :  [[-179.40472]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[0]]\n",
      "predictions :  [[-3.0133026]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1452.3346]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1854.8844]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1328.0775]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[904.61273]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1575.7936]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2239.7954]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1155.3954]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1145.9911]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[2125.059]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1696.9991]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1800.8167]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1333.5363]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[779.65594]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[320.1591]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1279.6749]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[0]]\n",
      "predictions :  [[-102.487564]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1258.7098]]\n",
      "------ digging more -------\n",
      "nickName :  el512\n",
      "postNumber :  222008681533\n",
      "blogUrl :  https://m.blog.naver.com/el512?Redirect=Log&logNo=222008681533\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1207.4253]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1541.6589]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1634.3007]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[493.76627]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1447.7504]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[439.6809]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1175.1599]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[996.2176]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[720.71014]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1016.7123]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1360.9524]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1015.6047]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[903.1925]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[950.27295]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1127.1924]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1162.3496]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1040.393]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1107.1628]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[550.438]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1115.7103]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1370.6677]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1131.4492]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1473.7072]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[0]]\n",
      "predictions :  [[-8.888512]]\n",
      "------ digging more -------\n",
      "nickName :  p_radn\n",
      "postNumber :  221878088433\n",
      "blogUrl :  https://m.blog.naver.com/p_radn?Redirect=Log&logNo=221878088433\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1621.2133]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[1385.22]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[523.37537]]\n",
      "========pridiction[Without Download]========\n",
      "predictions class :  [[1]]\n",
      "predictions :  [[997.1385]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_input to have shape (260, 460, 3) but got array with shape (260, 460, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-5e6e335595e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfindBlogImgGethering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearchQuery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msearchOption\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-1bc51fad999c>\u001b[0m in \u001b[0;36mfindBlogImgGethering\u001b[0;34m(searchQuery, searchOption, page)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;31m#isSaveSuccess = saveImage(False,index,img_url,nickName,postNumber)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;31m#np arr from img_url for validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mPredictionWithoutDownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;31m#                 if(isSaveSuccess):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;31m#                     index = index+1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-9d1bf4ca26f5>\u001b[0m in \u001b[0;36mPredictionWithoutDownload\u001b[0;34m(uri)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0minput_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert single image to a batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mpredictions_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"========pridiction[Without Download]========\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mpredict_classes\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \"\"\"\n\u001b[0;32m--> 338\u001b[0;31m     \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[0;32m--> 646\u001b[0;31m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    580\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    583\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_input to have shape (260, 460, 3) but got array with shape (260, 460, 1)"
     ]
    }
   ],
   "source": [
    "searchQuery = \"종각 맛집\"\n",
    "searchOption = 'sim'  #sim or date  \n",
    "page = 1\n",
    "\n",
    "findBlogImgGethering(searchQuery,searchOption,page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
